<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ruian He @ FDU</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="favicon.ico">
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VLTBF98M9J"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-VLTBF98M9J');
  </script>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ruian He | 何瑞安</name>
              </p>
              <p>I am a 3rd-year PhD student in School of Computer Science at <a href="https://www.fudan.edu.cn">Fudan University</a>, supervised by <a href="https://dml.fudan.edu.cn/87/22/c35294a427810/page.htm">Prof. Bo Yan</a>. Before that, I received my Bachelor's degree of Computer Science at Fudan University in 2021. My research interests lie in the field of low-level vision, with application to rendering and microscopy imaging. 
              </p>
              <p>Email: rahe16 (at) fudan.edu.cn.
              </p>
              <p style="text-align:left">
                <!-- <a href="mailto:rahe21@fudan.edu.cn">Email</a> &nbsp/&nbsp -->
                <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=dvNlwU0AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.semanticscholar.org/author/Ruian-He/2118180466">Semantic Scholar</a> &nbsp/&nbsp 
                <a href="https://dblp.uni-trier.de/pid/297/3167.html">DBLP</a> &nbsp/&nbsp 
                <a href="https://github.com/ryanhe312">Github</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/ryan-59-74">Zhihu</a> &nbsp/&nbsp
                <a href="https://space.bilibili.com/218032540">Bilibili</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <ul>
                  <li> <b>[July 2024]</b> One paper has been accepted to ACMMM 2024.</li>

                  <!-- <a onclick="return display('old_news');"> ---- show more ----</a>
                  <div id="old_news" style="display: none;">
                    <li> <b>[Oct. 2021]</b> I gave a talk on efficient NLP at <a href="https://www.bilibili.com/video/BV1NL4y1q7tT?vd_source=c04e95624eff313d107e25d2561c2484">BAAI
                        Big Model Meetup</a>. </li>

                  </div> -->
                </ul>
              </td>
            </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>(*: Equal contribution)
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/ACMMM24_FacialFlowNet.png" alt="clean-usnob" width="320" height="120">
            </td>  
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=xsx39qJaLq">
                <papertitle>FacialFlowNet: Advancing Facial Optical Flow Estimation with a Diverse Dataset and a Decomposed Model</papertitle>
              </a>
              <br>
              Jianzhi Lu*, <strong>Ruian He*</strong>, Shili Zhou, Weimin Tan, Bo Yan
              <br>
              <em>ACMMM</em>, 2024
              <p> We proposes FacialFlowNet, a novel large-scale facial optical flow dataset, and the Decomposed Facial Flow Model, the first method capable of decomposing facial flow.  
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/NM2024_UniFMIR.jpg" alt="clean-usnob" width="320" height="200">
            </td>  
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://www.nature.com/articles/s41592-024-02244-3">
                <papertitle>Pre-training a Foundation Model for Generalizable Fluorescence Microscopy-Based Image Restoration</papertitle>
              </a>
              <br>
              Chenxi Ma*, Weimin Tan*, <strong>Ruian He</strong>, Bo Yan
              <br>
              <em>Nature Methods</em>, 2024
              <p> We provide a universal fluorescence microscopy-based image restoration (UniFMIR) model to address different restoration problems, and show that UniFMIR offers higher image restoration precision, better generalization and increased versatility. 
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/AAAI24_STSSNet.png" alt="clean-usnob" width="320" height="200">
            </td>  
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.10890">
                <papertitle>Low-latency Space-time Supersampling for Real-time Rendering</papertitle>
              </a>
              <br>
              <strong>Ruian He*</strong>, Shili Zhou*, Yuqi Sun, Ri Cheng, Weimin Tan , Bo Yan
              <br>
              <em>AAAI</em>, 2024
              <p> We recognize the shared context and mechanisms between frame supersampling and extrapolation, save up to 75% of time against the conventional two-stage pipeline that necessitates 17ms.
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/AAAI24_SAMFlow.png" alt="clean-usnob" width="320" height="200">
            </td>
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://arxiv.org/abs/2307.16586">
                <papertitle>SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model</papertitle>
              </a>
              <br>
              Shili Zhou, <strong>Ruian He</strong>, Weimin Tan , Bo Yan
              <br>
              <em>AAAI</em>, 2024
              <p> Through theoretical analysis, we find the pre-trained large vision models are helpful in optical flow estimation, and SAM is suitable for solving the fragmentation problem. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/AAAI24_CAIP.png" alt="clean-usnob" width="320" height="200">
            </td>
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.07180">
                <papertitle>Context-Aware Iteration Policy Network for Efficient Optical Flow Estimation</papertitle>
              </a>
              <br>
              Ri Cheng, <strong>Ruian He</strong>, Xuhao Jiang, Shili Zhou, Weimin Tan , Bo Yan
              <br>
              <em>AAAI</em>, 2024
              <p>  We develop a Context-Aware Iteration Policy Network, which determines the optimal number of iterations per sample and reduce FLOPs by about 40%/20% for the Sintel/KITTI datasets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/ACMMM23_UGSP.png" alt="clean-usnob" width="320" height="250">
            </td>
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611752">
                <papertitle>Uncertainty-Guided Spatial Pruning Architecture for Efficient Frame Interpolation</papertitle>
              </a>
              <br>
              Ri Cheng , Xuhao Jiang , <strong>Ruian He</strong>, Shili Zhou , Weimin Tan , Bo Yan
              <br>
              <em>ACMMM</em>, 2023
              <p> We develop an Uncertainty-Guided Spatial Pruning (UGSP) architecture to skip redundant computation for efficient frame interpolation dynamically.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/ACMMM23_MVFlow.png" alt="clean-usnob" width="320" height="150">
            </td>
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611750">
                <papertitle>MVFlow: Deep Optical Flow Estimation of Compressed Videos with Motion Vector Prior</papertitle>
              </a>
              <br>
              Shili Zhou*, Xuhao Jiang*, Weimin Tan, <strong>Ruian He</strong>, Bo Yan
              <br>
              <em>ACMMM</em>, 2023
              <p> We propose an optical flow model, MVFlow, which uses motion vectors to improve the speed and accuracy of optical flow estimation for compressed videos.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/CVPRW23_Anti-UAV.png" alt="clean-usnob" width="320" height="200">
            </td>
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/He_Motion_Matters_Difference-Based_Multi-Scale_Learning_for_Infrared_UAV_Detection_CVPRW_2023_paper.html">
                <papertitle>Motion Matters: Difference-based Multi-scale Learning for Infrared UAV Detection</papertitle>
              </a>
              <br>
              <strong>Ruian He</strong>, Shili Zhou, Ri Cheng, Yuqi Sun, Weimin Tan, Bo Yan
              <br>
              <em>The 3nd Anti-UAV Workshop & Challenge - CVPR Workshops</em>, 2023
              <p> Our method utilizes the frame difference of multiple previous frames and fuse multiple spatial-temporal scales.</p>
            </td>
          </tr>
				
          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/ICASSP23_Fine-grained.png" alt="clean-usnob" width="320" height="160">
            </td>
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10097082/">
                <papertitle>Fine-grained Blind Face Inpainting with 3D Face Component Disentanglement</papertitle>
              </a>
              <br>
              Yu Bai, <strong>Ruian He</strong>, Weimin Tan, Bo Yan, Yangle Lin
              <br>
              <em>ICASSP</em>, 2023
              <p> We propose a novel fine-grained blind face inpainting framework and build up a new dataset called CelebO-3D.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;vertical-align:middle" align="center">
              <img src="images/ACMMM22_Co-Completion.png" alt="clean-usnob" width="320" height="160">
            </td>
          </tr><tr>
            <td style="padding:20px;vertical-align:middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548183">
                <papertitle>Co-completion for occluded facial expression recognition</papertitle>
              </a>
              <br>
              Zhen Xing*, Weimin Tan*, <strong>Ruian He</strong>, Yangle Lin, Bo Yan
              <br>
              <em>ACMMM</em>, 2022
              <p> We propose a task-specific framework which first combines occlusion discarding and feature completion together to reduce the interference of occlusions on instance level.</p>
            </td>
          </tr>

        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Projects</heading>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/ACMMM22_Co-Completion.png" alt="clean-usnob" width="320" height="160">
          </td>
          <td width="75%" valign="middle">
            <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548183">
              <papertitle>Co-completion for occluded facial expression recognition</papertitle>
            </a>
            <br>
            Zhen Xing*, Weimin Tan*, <strong>Ruian He</strong>, Yangle Lin, Bo Yan
            <br>
            <em>Proceedings of the 30th ACM International Conference on Multimedia</em>, 130–140, 2022
            <p> In this paper, we propose Co-Completion,
              a task-specific framework which first combines occlusion discarding and feature completion together to reduce the interference of
              occlusions on instance level.</p>
          </td>
        </tr>
        
        </tbody></table> -->
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Service</heading>
                <ul>
                  <li>
                    Conference Reviewer / Program Committee Member for CVPR (2022, 2024), NIPS (2024), ICCV (2023), ECCV (2024), AAAI (2023, 2024), IEEEVR (2023), ACMMM (2024).
                  </li>
                  <li>
                    Journal Reviewer for TPAMI (2024), TCSVT (2024). 
                  </li>
                  <li>
                    Teaching Assistant for COMP130018.01 Computer Graphics A (2023 Spring, 2024 Spring), COMP130014.01 Compiler (2022 Autumn). 
                  </li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Prizes</heading>
                <ul>
                  <li> Second place in <a href="https://anti-uav.github.io/leaderboard3/index.html">the 3rd Anti-UAV Challenge</a> in CVPR 2023. </li>
                  <li> Third Prize in <a href="https://iacc.pazhoulab-huangpu.com/contestdetail?id=64af6ccc4a0ed647faca76ee&award=1,000,000"> Fast-Motion VFI Challenge</a> in IACC 2023. </li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Awards</heading>
                <ul>
                  <li> Jinrirencai Scholarship (10 students in Fudan). 2024. </li>
                  <li> Huawei Scholarship (Top 10%). 2022. </li>
		              <li> Award for Outstanding Ph.D. Students. 2021, 2023. </li>
                  <li> Boxue Scholarship (Top 10%). 2017. </li>
                  <li> Award for Outstanding Undergraduate Students. 2018, 2019, 2020. </li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:0px">
                <p font-size:small;="">
                  <br>
                  <br>
                </p><div style="float:left;">
                  Updated at July 2024.
                </div>
                <div style="float:right;">
                  <a href="https://jonbarron.info">Template</a>
                </div>
                <br>
                <br>
                <p></p>
              </td>
            </tr>
          </tbody>
        </table>


      </td>
    </tr>
  </table>
</body>

</html>
