<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ruian He @ FDU</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="favicon.ico">
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VLTBF98M9J"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-VLTBF98M9J');
  </script>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ruian He | 何瑞安</name>
              </p>
              <p>I am a 3nd-year PhD student in School of Computer Science at <a href="https://www.fudan.edu.cn">Fudan University</a>, supervised by <a href="https://dml.fudan.edu.cn/87/22/c35294a427810/page.htm">Prof. Bo Yan</a>. Before that, I received my Bachelor's degree of Computer Science at Fudan University in 2021.
              </p>
              <p>My research interests lie in the field of computer vision and computer graphics. I have explored a lot of topics, such as facial representation learning, optical flow estimation, video synthesis, and postprocessing of rendering. 
              </p>
              <p>Email: rahe21 (at) m.fudan.edu.cn.
              </p>
              <p style="text-align:left">
                <!-- <a href="mailto:rahe21@fudan.edu.cn">Email</a> &nbsp/&nbsp -->
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=dvNlwU0AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.semanticscholar.org/author/Ruian-He/2118180466">Semantic Scholar</a> &nbsp/&nbsp 
                <a href="https://dblp.uni-trier.de/pid/297/3167.html">DBLP</a> &nbsp/&nbsp 
                <a href="https://github.com/ryanhe312">Github</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/ryan-59-74">Zhihu</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <ul>
                  <li> <b>[Dec. 2023]</b> Three papers have been accepted to AAAI 2024.</li>

                  <!-- <a onclick="return display('old_news');"> ---- show more ----</a>
                  <div id="old_news" style="display: none;">
                    <li> <b>[Oct. 2021]</b> I gave a talk on efficient NLP at <a href="https://www.bilibili.com/video/BV1NL4y1q7tT?vd_source=c04e95624eff313d107e25d2561c2484">BAAI
                        Big Model Meetup</a>. </li>

                  </div> -->
                </ul>
              </td>
            </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>(*: Equal contribution)
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/AAAI24_STSSNet.png" alt="clean-usnob" width="320" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2312.10890">
                <papertitle>Low-latency Space-time Supersampling for Real-time Rendering</papertitle>
              </a>
              <br>
              <strong>Ruian He*</strong>, Shili Zhou*, Yuqi Sun, Ri Cheng, Weimin Tan , Bo Yan
              <br>
              <em>AAAI</em>, 2024
              <p> In this paper, we recognize the shared context and mechanisms between frame supersampling and extrapolation, and present a novel framework, Space-time Supersampling (STSS). To implement an efficient architecture, we treat the aliasing and warping holes unified as reshading regions and put forth two key components to compensate the regions, namely Random Reshading Masking (RRM) and Efficient Reshading Module (ERM). Notably, the performance is achieved within only 4ms, saving up to 75\% of time against the conventional two-stage pipeline that necessitates 17ms.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/AAAI24_SAMFlow.png" alt="clean-usnob" width="320" height="350">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2307.16586">
                <papertitle>SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model</papertitle>
              </a>
              <br>
              Shili Zhou, <strong>Ruian He</strong>, Weimin Tan , Bo Yan
              <br>
              <em>AAAI</em>, 2024
              <p> Optical Flow Estimation aims to find the 2D dense motion field between two frames. Due to the limitation of model structures and training datasets, existing methods often rely too much on local clues and ignore the integrity of objects, resulting in fragmented motion estimation. Through theoretical analysis, we find the pre-trained large vision models are helpful in optical flow estimation, and we notice that the recently famous Segment Anything Model (SAM) demonstrates a strong ability to segment complete objects, which is suitable for solving the fragmentation problem. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/AAAI24_CAIP.png" alt="clean-usnob" width="320" height="200">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2312.07180">
                <papertitle>Context-Aware Iteration Policy Network for Efficient Optical Flow Estimation</papertitle>
              </a>
              <br>
              Ri Cheng, <strong>Ruian He</strong>, Xuhao Jiang, Shili Zhou, Weimin Tan , Bo Yan
              <br>
              <em>AAAI</em>, 2024
              <p>  In this paper, we develop a Context-Aware Iteration Policy Network for efficient optical flow estimation, which determines the optimal number of iterations per sample. The policy network achieves this by learning contextual information to realize whether flow improvement is bottlenecked or minimal. Our policy network can be easily integrated into state-of-the-art optical flow networks. Extensive experiments show that our method maintains performance while reducing FLOPs by about 40%/20% for the Sintel/KITTI datasets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ACMMM23_UGSP.png" alt="clean-usnob" width="320" height="250">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611752">
                <papertitle>Uncertainty-Guided Spatial Pruning Architecture for Efficient Frame Interpolation</papertitle>
              </a>
              <br>
              Ri Cheng , Xuhao Jiang , <strong>Ruian He</strong>, Shili Zhou , Weimin Tan , Bo Yan
              <br>
              <em>ACMMM</em>, 2023
              <p> The video frame interpolation (VFI) model applies the convolution operation to all locations, leading to redundant computations in regions with easy motion. We can use dynamic spatial pruning method to skip redundant computation, but this method cannot properly identify easy regions in VFI tasks without supervision. In this paper, we develop an Uncertainty-Guided Spatial Pruning (UGSP) architecture to skip redundant computation for efficient frame interpolation dynamically.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ACMMM23_MVFlow.png" alt="clean-usnob" width="320" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611750">
                <papertitle>MVFlow: Deep Optical Flow Estimation of Compressed Videos with Motion Vector Prior</papertitle>
              </a>
              <br>
              Shili Zhou*, Xuhao Jiang*, Weimin Tan, <strong>Ruian He</strong>, Bo Yan
              <br>
              <em>ACMMM</em>, 2023
              <p> Motion vectors, one of the compression information, record the motion of the video frames. They can be directly extracted from the compression code stream without computational cost and serve as a solid prior for optical flow estimation. Therefore, we propose an optical flow model, MVFlow, which uses motion vectors to improve the speed and accuracy of optical flow estimation for compressed videos.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CVPRW23_Anti-UAV.png" alt="clean-usnob" width="320" height="200">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/He_Motion_Matters_Difference-Based_Multi-Scale_Learning_for_Infrared_UAV_Detection_CVPRW_2023_paper.html">
                <papertitle>Motion Matters: Difference-based Multi-scale Learning for Infrared UAV Detection</papertitle>
              </a>
              <br>
              <strong>Ruian He</strong>, Shili Zhou, Ri Cheng, Yuqi Sun, Weimin Tan, Bo Yan
              <br>
              <em>The 3nd Anti-UAV Workshop & Challenge - CVPR Workshops</em>, 2023
              <p> In this paper, we propose a novel learning framework for robust UAV detectors, which we call Difference-based Multi-scale Learning (DML). Our method utilizes the frame difference of multiple previous frames, extracting motion information and blocking background noise. We also fuse multiple spatial-temporal scales for training and inferencing, enabling fusion from different sources.</p>
            </td>
          </tr>
				
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ICASSP23_Fine-grained.png" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/10097082/">
                <papertitle>Fine-grained Blind Face Inpainting with 3D Face Component Disentanglement</papertitle>
              </a>
              <br>
              Yu Bai, <strong>Ruian He</strong>, Weimin Tan, Bo Yan, Yangle Lin
              <br>
              <em>ICASSP</em>, 2023
              <p> In this paper, we propose a novel fine-grained blind face inpainting framework, combining 3D face components disentanglement with generative network. We also build up a new dataset called CelebO-3D which consists of occluded face images synthesized with 3D occlusion and rendered by 3D face model.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ACMMM22_Co-Completion.png" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548183">
                <papertitle>Co-completion for occluded facial expression recognition</papertitle>
              </a>
              <br>
              Zhen Xing*, Weimin Tan*, <strong>Ruian He</strong>, Yangle Lin, Bo Yan
              <br>
              <em>ACMMM</em>, 2022
              <p> In this paper, we propose Co-Completion,
                a task-specific framework which first combines occlusion discarding and feature completion together to reduce the interference of
                occlusions on instance level.</p>
            </td>
          </tr>

        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Projects</heading>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/ACMMM22_Co-Completion.png" alt="clean-usnob" width="320" height="160">
          </td>
          <td width="75%" valign="middle">
            <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548183">
              <papertitle>Co-completion for occluded facial expression recognition</papertitle>
            </a>
            <br>
            Zhen Xing*, Weimin Tan*, <strong>Ruian He</strong>, Yangle Lin, Bo Yan
            <br>
            <em>Proceedings of the 30th ACM International Conference on Multimedia</em>, 130–140, 2022
            <p> In this paper, we propose Co-Completion,
              a task-specific framework which first combines occlusion discarding and feature completion together to reduce the interference of
              occlusions on instance level.</p>
          </td>
        </tr>
        
        </tbody></table> -->
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Service</heading>
                <p>
                  <b>Reviewer / Program Committee Member</b>
                </p>
                <ul>
                  <li> AAAI (2023, 2024)</li>
                  <li> CVPR (2022, 2024)</li>
                  <li> ICCV (2023)</li>
                  <li> IEEEVR (2023)</li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Prize</heading>
                <ul>
                  <li> Second place in <a href="https://anti-uav.github.io/leaderboard3/index.html">the 3rd Anti-UAV Challenge</a> in CVPR 2023. </li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Awards</heading>
                <ul>
                  <li> Huawei Scholarship (Top 10%). 2022. </li>
                  <li> Fudan Student Scholarship. 2018, 2019, 2020, 2021. </li>
                  <li> Boxue Scholarship (Top 10%). 2017.</li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:0px">
                <p font-size:small;="">
                  <br>
                  <br>
                </p><div style="float:left;">
                  Updated at Jan. 2024.
                </div>
                <div style="float:right;">
                  <a href="https://jonbarron.info">Template</a>
                </div>
                <br>
                <br>
                <p></p>
              </td>
            </tr>
          </tbody>
        </table>


      </td>
    </tr>
  </table>
</body>

</html>
